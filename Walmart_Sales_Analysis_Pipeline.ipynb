{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7PIdHhUCElIgregifUPCc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyasankar99/Walmart-Sales-Datapipeline/blob/main/Walmart_Sales_Analysis_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install PySpark and Java"
      ],
      "metadata": {
        "id": "nBydNUWyWEGc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkVyzjzNV6fi"
      },
      "outputs": [],
      "source": [
        "# Installing PySpark and Java\n",
        "# Installing  Java and the compatible version of PySpark\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download Spark (adjusting the version when needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz -O spark-3.1.2-bin-hadoop3.2.tgz\n",
        "\n",
        "# Unzip the downloaded file\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the Environment Variables"
      ],
      "metadata": {
        "id": "f_qOuREgWY44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuring the environment for Java and Spark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n"
      ],
      "metadata": {
        "id": "LyiFbKrmV8CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.1.2\n"
      ],
      "metadata": {
        "id": "rPP6CLUqZOt7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b172404-0b11-47ab-9737-79ae80832a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.1.2 in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.1.2) (0.10.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "print(\"PySpark version:\", pyspark.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mr-hBIECX4Yu",
        "outputId": "59eff7cc-4c28-4266-b656-6c4ef7a95762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PySpark version: 3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Spark Session"
      ],
      "metadata": {
        "id": "HweEofPUWz6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"WalmartSalesDataPipeline\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "Efdl_X9UV8Ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Data"
      ],
      "metadata": {
        "id": "ar_QGsixumbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data into DataFrames\n",
        "#customers_df = spark.read.csv(\"/content/customers.tsv\", header=True, inferSchema=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "rA3egHPCuZjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"AddHeadersToTSV\").getOrCreate()\n",
        "\n",
        "# Define the schema with the required column names and data types\n",
        "schema = StructType([\n",
        "    StructField(\"Customer_ID\", StringType(), True),\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"City\", StringType(), True),\n",
        "    StructField(\"State\", StringType(), True),\n",
        "    StructField(\"Zip_Code\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Load TSV data with the defined schema\n",
        "file_path = \"/content/customers.tsv\"\n",
        "customers_df = spark.read.option(\"delimiter\", \"\\t\").csv(file_path, schema=schema, header=False)\n"
      ],
      "metadata": {
        "id": "FuRBd3r7vWXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the result\n",
        "customers_df.show()"
      ],
      "metadata": {
        "id": "6qKyCaY6MbW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c74f007-c526-4a72-9a25-131ac34b5f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------+-------------+-----+--------+\n",
            "|Customer_ID|            Name|         City|State|Zip_Code|\n",
            "+-----------+----------------+-------------+-----+--------+\n",
            "|      11039|     Mary Torres|       Caguas|   PR|     725|\n",
            "|       5623|      Jose Haley|     Columbus|   OH|   43207|\n",
            "|       5829|      Mary Smith|      Houston|   TX|   77015|\n",
            "|       6336|  Richard Maddox|       Caguas|   PR|     725|\n",
            "|       1708|  Margaret Booth|    Arlington|   TX|   76010|\n",
            "|      10227|  Mary Henderson|       Caguas|   PR|     725|\n",
            "|        839|     Lisa Walker|       Caguas|   PR|     725|\n",
            "|       7604|   Jonathan Hill|      Phoenix|   AZ|   85040|\n",
            "|       6485|Carolyn Sheppard|Pompano Beach|   FL|   33063|\n",
            "|       4737|    Mary Mendoza|       Caguas|   PR|     725|\n",
            "|       5973|   Michael Smith|       Caguas|   PR|     725|\n",
            "|       9205|    James Holmes|     Hilliard|   OH|   43026|\n",
            "|        138|     Mary Dawson|       Caguas|   PR|     725|\n",
            "|        371|    Adam Marquez|  San Antonio|   TX|   78223|\n",
            "|       9285|    Gloria Smith|       Caguas|   PR|     725|\n",
            "|       1209|       Mary Webb|   San Marcos|   TX|   78666|\n",
            "|       3021|  Nancy Alvarado|     Flushing|   NY|   11354|\n",
            "|       3354|  Russell Flores|       Caguas|   PR|     725|\n",
            "|      11684|    Denise Smith|    Rego Park|   NY|   11374|\n",
            "|      11144|  Jose Dickerson|         Mesa|   AZ|   85201|\n",
            "+-----------+----------------+-------------+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sales_df = spark.read.csv(\"/content/salestxns.tsv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "Mle7Lf9TuUr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"AddHeadersToTSV\").getOrCreate()\n",
        "\n",
        "# Define the schema with the required column names and data types\n",
        "schema = StructType([\n",
        "    StructField(\"Sales_Txn_ID\", StringType(), True),\n",
        "    StructField(\"Category_ID\", StringType(), True),\n",
        "    StructField(\"Category_Name\", StringType(), True),\n",
        "    StructField(\"Product_ID\", StringType(), True),\n",
        "    StructField(\"Product_Name\", StringType(), True),\n",
        "    StructField(\"Price\", StringType(), True),\n",
        "    StructField(\"Quantity\", StringType(), True),\n",
        "    StructField(\"Customer_ID\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Load TSV data with the defined schema\n",
        "file_path = \"/content/salestxns.tsv\"\n",
        "sales_df = spark.read.option(\"delimiter\", \"\\t\").csv(file_path, schema=schema, header=False)"
      ],
      "metadata": {
        "id": "RDTU-6FSyzq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.show()"
      ],
      "metadata": {
        "id": "W9OdfDbPMkcQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "952e184e-36ff-47da-b375-fd6697b9e319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+--------------------+----------+--------------------+------+--------+-----------+\n",
            "|Sales_Txn_ID|Category_ID|       Category_Name|Product_ID|        Product_Name| Price|Quantity|Customer_ID|\n",
            "+------------+-----------+--------------------+----------+--------------------+------+--------+-----------+\n",
            "|           1|         43|    Camping & Hiking|       957|Diamondback Women...|299.98|       1|      11599|\n",
            "|           2|         48|        Water Sports|      1073|Pelican Sunstream...|199.99|       1|        256|\n",
            "|           3|         24|     Women's Apparel|       502|Nike Men's Dri-FI...|    50|       5|        256|\n",
            "|           4|         18|      Men's Footwear|       403|Nike Men's CJ Eli...|129.99|       1|        256|\n",
            "|           5|         40|         Accessories|       897|Team Golf New Eng...| 24.99|       2|       8827|\n",
            "|           6|         17|              Cleats|       365|Perfect Fitness P...| 59.99|       5|       8827|\n",
            "|           7|         24|     Women's Apparel|       502|Nike Men's Dri-FI...|    50|       3|       8827|\n",
            "|           8|         46|Indoor/Outdoor Games|      1014|O'Brien Men's Neo...| 49.98|       4|       8827|\n",
            "|           9|         43|    Camping & Hiking|       957|Diamondback Women...|299.98|       1|      11318|\n",
            "|          10|         17|              Cleats|       365|Perfect Fitness P...| 59.99|       5|      11318|\n",
            "|          11|         46|Indoor/Outdoor Games|      1014|O'Brien Men's Neo...| 49.98|       2|      11318|\n",
            "|          12|         43|    Camping & Hiking|       957|Diamondback Women...|299.98|       1|      11318|\n",
            "|          13|         18|      Men's Footwear|       403|Nike Men's CJ Eli...|129.99|       1|      11318|\n",
            "|          14|         48|        Water Sports|      1073|Pelican Sunstream...|199.99|       1|       4530|\n",
            "|          15|         43|    Camping & Hiking|       957|Diamondback Women...|299.98|       1|       4530|\n",
            "|          16|         41|            Trade-In|       926|Glove It Imperial...| 15.99|       5|       4530|\n",
            "|          17|         17|              Cleats|       365|Perfect Fitness P...| 59.99|       3|       2911|\n",
            "|          18|         17|              Cleats|       365|Perfect Fitness P...| 59.99|       5|       2911|\n",
            "|          19|         46|Indoor/Outdoor Games|      1014|O'Brien Men's Neo...| 49.98|       4|       2911|\n",
            "|          20|         24|     Women's Apparel|       502|Nike Men's Dri-FI...|    50|       1|       2911|\n",
            "+------------+-----------+--------------------+----------+--------------------+------+--------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing ✅\n",
        "\n"
      ],
      "metadata": {
        "id": "Ay3v0rAi2OAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checking for Data Nullity\n",
        "\n",
        "- Creating a function that calculates the count of nulls for each column in a DataFrame."
      ],
      "metadata": {
        "id": "4q2ElD0r9dWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Function to count null values in each column\n",
        "def count_nulls(df):\n",
        "    return df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])\n"
      ],
      "metadata": {
        "id": "tA9GYWY-9qIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Applying this function to both datasets to get a summary of missing values for each column."
      ],
      "metadata": {
        "id": "LbOU2Jtl-LW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count missing values in customers dataset\n",
        "null_counts_customers = count_nulls(customers_df)\n",
        "null_counts_customers.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMaWzIuL-PX2",
        "outputId": "58d74a40-a215-4d98-ebc8-8c2965bc8e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----+----+-----+--------+\n",
            "|Customer_ID|Name|City|State|Zip_Code|\n",
            "+-----------+----+----+-----+--------+\n",
            "|          0|   0|   0|    0|       0|\n",
            "+-----------+----+----+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count missing values in sales dataset\n",
        "null_counts_sales = count_nulls(sales_df)\n",
        "null_counts_sales.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6i3LXCtnrc-",
        "outputId": "179eecf6-b9de-44bb-b608-7fc4a31c7c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+-------------+----------+------------+-----+--------+-----------+\n",
            "|Sales_Txn_ID|Category_ID|Category_Name|Product_ID|Product_Name|Price|Quantity|Customer_ID|\n",
            "+------------+-----------+-------------+----------+------------+-----+--------+-----------+\n",
            "|           0|          0|            0|         0|           0|    0|       0|          0|\n",
            "+------------+-----------+-------------+----------+------------+-----+--------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convert necessary columns to appropriate data types."
      ],
      "metadata": {
        "id": "IGhFlXx59bh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType, FloatType\n",
        "\n",
        "# Convert data types for necessary columns\n",
        "sales_df = sales_df.withColumn(\"Price\", sales_df[\"Price\"].cast(FloatType())) \\\n",
        "                   .withColumn(\"Quantity\", sales_df[\"Quantity\"].cast(IntegerType()))\n"
      ],
      "metadata": {
        "id": "rRSudtew0pyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SQL Queries ⚡"
      ],
      "metadata": {
        "id": "DBxHsO4W22Ft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Total Number of Customers:\n",
        "How many unique customers are there in the dataset?\n"
      ],
      "metadata": {
        "id": "N1lL4acD28zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_customers = customers_df.select(\"Customer_ID\").distinct().count()\n",
        "print(\"Total number of unique customers:\", unique_customers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31MlHQGH0pdl",
        "outputId": "8f9d8eeb-26f4-469b-b870-a9b2717e2537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique customers: 1244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.Total Sales by State:\n",
        "What is the total sales amount for each state?"
      ],
      "metadata": {
        "id": "zgVsQ6JT3XP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Join sales and customers data\n",
        "joined_df = sales_df.join(customers_df, on=\"Customer_ID\", how=\"inner\")\n",
        "\n",
        "# Calculate total sales by state\n",
        "total_sales_by_state = joined_df.withColumn(\"Total_Price\", F.col(\"Price\") * F.col(\"Quantity\")) \\\n",
        "                                .groupBy(\"State\") \\\n",
        "                                .agg(F.sum(\"Total_Price\").alias(\"Total_Sales\"))\n",
        "\n",
        "total_sales_by_state.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPhUILAy3WqJ",
        "outputId": "337fa75b-ec1b-403a-fdad-7846ec4d4461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------------+\n",
            "|State|       Total_Sales|\n",
            "+-----+------------------+\n",
            "|   AZ|  48702.6809425354|\n",
            "|   SC| 4144.680107116699|\n",
            "|   LA| 24449.42046356201|\n",
            "|   MN| 3549.600028991699|\n",
            "|   NJ| 52303.09112358093|\n",
            "|   DC| 8798.760160446167|\n",
            "|   OR| 9544.780143737793|\n",
            "|   VA|30488.970476150513|\n",
            "|   RI| 5424.410140991211|\n",
            "|   KY| 2749.700065612793|\n",
            "|   MI| 83347.09171104431|\n",
            "|   NV| 47103.61082458496|\n",
            "|   WI| 24561.30038833618|\n",
            "|   ID|10098.950242996216|\n",
            "|   CA|503205.49938964844|\n",
            "|   CT|19206.770456314087|\n",
            "|   NC| 45275.89086151123|\n",
            "|   MD| 51982.49096298218|\n",
            "|   DE|1305.7600326538086|\n",
            "|   MO| 34749.06068229675|\n",
            "+-----+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Top 10 Most Purchased Products:\n",
        "Which are the top 10 most purchased products based on the quantity sold?"
      ],
      "metadata": {
        "id": "9NMnCHEk5W-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_products = sales_df.groupBy(\"Product_Name\") \\\n",
        "                       .agg(F.sum(\"Quantity\").alias(\"Total_Quantity\")) \\\n",
        "                       .orderBy(F.desc(\"Total_Quantity\")) \\\n",
        "                       .limit(10)\n",
        "\n",
        "top_products.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Eaxjppn5WfW",
        "outputId": "6427ef5b-5145-42e1-8d05-05e441e2b770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------+\n",
            "|        Product_Name|Total_Quantity|\n",
            "+--------------------+--------------+\n",
            "|Perfect Fitness P...|         73698|\n",
            "|Nike Men's Dri-FI...|         62956|\n",
            "|O'Brien Men's Neo...|         57803|\n",
            "|Nike Men's Free 5...|         36680|\n",
            "|Under Armour Girl...|         31735|\n",
            "|Nike Men's CJ Eli...|         22246|\n",
            "|Field & Stream Sp...|         17325|\n",
            "|Pelican Sunstream...|         15500|\n",
            "|Diamondback Women...|         13729|\n",
            "|ENO Atlas Hammock...|           998|\n",
            "+--------------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Average Transaction Value:\n",
        "What is the average price of transactions across all sales?\n"
      ],
      "metadata": {
        "id": "Vph-eb6U56N6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_transaction_value = sales_df.withColumn(\"Total_Price\", F.col(\"Price\") * F.col(\"Quantity\")) \\\n",
        "                                .agg(F.avg(\"Total_Price\").alias(\"Avg_Transaction_Value\"))\n",
        "\n",
        "avg_transaction_value.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-8Hl08M5569",
        "outputId": "a22af6fd-0acf-4851-bc43-39e053146fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+\n",
            "|Avg_Transaction_Value|\n",
            "+---------------------+\n",
            "|    199.3206689847089|\n",
            "+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Top 5 Customers by Expenditure:\n",
        "Who are the top 5 customers by total amount spent?"
      ],
      "metadata": {
        "id": "jUAIkv8m6Nj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_customers = joined_df.withColumn(\"Total_Price\", F.col(\"Price\") * F.col(\"Quantity\")) \\\n",
        "                         .groupBy(\"Customer_ID\", \"Name\") \\\n",
        "                         .agg(F.sum(\"Total_Price\").alias(\"Total_Expenditure\")) \\\n",
        "                         .orderBy(F.desc(\"Total_Expenditure\")) \\\n",
        "                         .limit(5)\n",
        "\n",
        "top_customers.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te10n59P6HSo",
        "outputId": "ed35c1e1-58c5-4975-e5e7-9046dc2c3d62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------------+-----------------+\n",
            "|Customer_ID|             Name|Total_Expenditure|\n",
            "+-----------+-----------------+-----------------+\n",
            "|       9371|   Mary Patterson|9299.030221939087|\n",
            "|        664|    Bobby Jimenez|8394.260208129883|\n",
            "|      12431|        Mary Rios|8073.150127410889|\n",
            "|      10591| Deborah Humphrey|7889.050121307373|\n",
            "|       9271|Christopher Smith|7665.250144958496|\n",
            "+-----------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Product Purchases by a Specific Customer:\n",
        "List all products purchased by a specific customer (e.g., customer with ID 256)"
      ],
      "metadata": {
        "id": "pt8KIeLQ6jPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_id = 256  # Example Customer ID\n",
        "\n",
        "customer_purchases = sales_df.filter(sales_df[\"Customer_ID\"] == customer_id) \\\n",
        "                             .select(\"Product_Name\", \"Quantity\", (F.col(\"Price\") * F.col(\"Quantity\")).alias(\"Total_Spent\"))\n",
        "\n",
        "customer_purchases.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaciZUe26G8M",
        "outputId": "faef2049-40bd-461b-8e8a-c79877a43eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------+-----------+\n",
            "|        Product_Name|Quantity|Total_Spent|\n",
            "+--------------------+--------+-----------+\n",
            "|Pelican Sunstream...|       1|     199.99|\n",
            "|Nike Men's Dri-FI...|       5|      250.0|\n",
            "|Nike Men's CJ Eli...|       1|     129.99|\n",
            "|Team Golf St. Lou...|       5|     124.95|\n",
            "|TYR Boys' Team Di...|       5|  199.95001|\n",
            "|Field & Stream Sp...|       1|     399.98|\n",
            "|Field & Stream Sp...|       1|     399.98|\n",
            "|Nike Men's Dri-FI...|       5|      250.0|\n",
            "|Nike Men's CJ Eli...|       1|     129.99|\n",
            "|Nike Men's CJ Eli...|       1|     129.99|\n",
            "|Perfect Fitness P...|       5|     299.95|\n",
            "|O'Brien Men's Neo...|       5|      249.9|\n",
            "|Nike Men's CJ Eli...|       1|     129.99|\n",
            "|O'Brien Men's Neo...|       4|     199.92|\n",
            "|Under Armour Wome...|       1|      54.97|\n",
            "|Nike Women's Temp...|       4|      120.0|\n",
            "|Nike Men's Dri-FI...|       1|       50.0|\n",
            "|Nike Men's Dri-FI...|       2|      100.0|\n",
            "|Nike Men's Dri-FI...|       2|      100.0|\n",
            "|Diamondback Women...|       1|     299.98|\n",
            "+--------------------+--------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Monthly Sales Trends:\n",
        "Assuming there is a date field, analyze the sales trends over the months. Which month had the highest sales?\n"
      ],
      "metadata": {
        "id": "50vRjkum7ifV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Since There is no 'Date' and 'Month' data column available in `Sales_df` dataset, so we can not extract Monthly Sales Data from it .\n",
        "\n",
        "- Since the document doesn’t indicate the presence of a date field in `sales_df`, we need to add a date column manually .\n",
        "\n",
        "- We will generate random dates within a specified range for each transaction, we can add a `Transaction_Date` column with dates spread randomly over a given range.\n",
        "\n",
        "\n",
        "So What the things we need to able to get the desired output...? ☝\n",
        "\n",
        "\n",
        "We can implement three things here:\n",
        "\n",
        "\n",
        "1) **Define the Date Range** Choose a start and end date.\n",
        "\n",
        "2)**Generate Random Dates** within that range for each row.\n",
        "\n",
        "3)**Extract the Month from the random** dates for monthly analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "3QbNM3vIBupu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import DateType\n",
        "from datetime import date, timedelta\n",
        "\n",
        "# Define the start and end dates for the random date range\n",
        "start_date = date(2023, 1, 1)\n",
        "end_date = date(2023, 12, 31)\n",
        "\n",
        "# Function to generate a random date within the specified range\n",
        "def random_date(start, end):\n",
        "    delta = end - start\n",
        "    random_days = random.randint(0, delta.days)\n",
        "    return start + timedelta(days=random_days)\n",
        "\n",
        "# Register the function as a UDF\n",
        "random_date_udf = F.udf(lambda: random_date(start_date, end_date), DateType())   # UDF = User Defined Function\n",
        "\n",
        "# Add a Transaction_Date column with random dates for each transaction\n",
        "sales_df = sales_df.withColumn(\"Transaction_Date\", random_date_udf())\n",
        "\n",
        "# Extract the month from the new Transaction_Date column\n",
        "sales_df = sales_df.withColumn(\"Month\", F.month(\"Transaction_Date\"))\n",
        "\n",
        "# Calculate monthly sales\n",
        "monthly_sales = sales_df.withColumn(\"Total_Price\", F.col(\"Price\") * F.col(\"Quantity\")) \\\n",
        "                        .groupBy(\"Month\") \\\n",
        "                        .agg(F.sum(\"Total_Price\").alias(\"Monthly_Sales\")) \\\n",
        "                        .orderBy(F.desc(\"Monthly_Sales\"))\n",
        "\n",
        "# Show the result\n",
        "sales_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P2eVQErBjSL",
        "outputId": "f85729ee-f556-4d3e-dfa8-2c8459040f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+--------------------+----------+--------------------+------+--------+-----------+----------------+-----+\n",
            "|Sales_Txn_ID|Category_ID|       Category_Name|Product_ID|        Product_Name| Price|Quantity|Customer_ID|Transaction_Date|Month|\n",
            "+------------+-----------+--------------------+----------+--------------------+------+--------+-----------+----------------+-----+\n",
            "|           1|         43|    Camping & Hiking|       957|Diamondback Women...|299.98|       1|      11599|      2023-08-20|    8|\n",
            "|           2|         48|        Water Sports|      1073|Pelican Sunstream...|199.99|       1|        256|      2023-12-01|   12|\n",
            "|           3|         24|     Women's Apparel|       502|Nike Men's Dri-FI...|  50.0|       5|        256|      2023-03-20|    3|\n",
            "|           4|         18|      Men's Footwear|       403|Nike Men's CJ Eli...|129.99|       1|        256|      2023-12-16|   12|\n",
            "|           5|         40|         Accessories|       897|Team Golf New Eng...| 24.99|       2|       8827|      2023-04-28|    4|\n",
            "|           6|         17|              Cleats|       365|Perfect Fitness P...| 59.99|       5|       8827|      2023-10-12|   10|\n",
            "|           7|         24|     Women's Apparel|       502|Nike Men's Dri-FI...|  50.0|       3|       8827|      2023-09-02|    9|\n",
            "|           8|         46|Indoor/Outdoor Games|      1014|O'Brien Men's Neo...| 49.98|       4|       8827|      2023-09-01|    9|\n",
            "|           9|         43|    Camping & Hiking|       957|Diamondback Women...|299.98|       1|      11318|      2023-07-08|    7|\n",
            "|          10|         17|              Cleats|       365|Perfect Fitness P...| 59.99|       5|      11318|      2023-02-28|    2|\n",
            "|          11|         46|Indoor/Outdoor Games|      1014|O'Brien Men's Neo...| 49.98|       2|      11318|      2023-01-10|    1|\n",
            "|          12|         43|    Camping & Hiking|       957|Diamondback Women...|299.98|       1|      11318|      2023-09-27|    9|\n",
            "|          13|         18|      Men's Footwear|       403|Nike Men's CJ Eli...|129.99|       1|      11318|      2023-07-19|    7|\n",
            "|          14|         48|        Water Sports|      1073|Pelican Sunstream...|199.99|       1|       4530|      2023-04-21|    4|\n",
            "|          15|         43|    Camping & Hiking|       957|Diamondback Women...|299.98|       1|       4530|      2023-07-19|    7|\n",
            "|          16|         41|            Trade-In|       926|Glove It Imperial...| 15.99|       5|       4530|      2023-04-05|    4|\n",
            "|          17|         17|              Cleats|       365|Perfect Fitness P...| 59.99|       3|       2911|      2023-04-22|    4|\n",
            "|          18|         17|              Cleats|       365|Perfect Fitness P...| 59.99|       5|       2911|      2023-09-26|    9|\n",
            "|          19|         46|Indoor/Outdoor Games|      1014|O'Brien Men's Neo...| 49.98|       4|       2911|      2023-10-04|   10|\n",
            "|          20|         24|     Women's Apparel|       502|Nike Men's Dri-FI...|  50.0|       1|       2911|      2023-07-06|    7|\n",
            "+------------+-----------+--------------------+----------+--------------------+------+--------+-----------+----------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate monthly sales\n",
        "monthly_sales = sales_df.withColumn(\"Total_Price\", F.col(\"Price\") * F.col(\"Quantity\")) \\\n",
        "                        .groupBy(\"Month\") \\\n",
        "                        .agg(F.sum(\"Total_Price\").alias(\"Monthly_Sales\")) \\\n",
        "                        .orderBy(F.desc(\"Monthly_Sales\"))\n",
        "\n",
        "# Show the result\n",
        "monthly_sales.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvK8zkF63_kM",
        "outputId": "c4a53086-d475-4fdf-8ad0-69c3e82b7384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------------+\n",
            "|Month|     Monthly_Sales|\n",
            "+-----+------------------+\n",
            "|    8|2958404.6940670013|\n",
            "|    3|2943308.9335212708|\n",
            "|    1| 2935582.263519287|\n",
            "|   12| 2912767.702472687|\n",
            "|    7| 2908226.104948044|\n",
            "|   10|2891314.8834323883|\n",
            "|    5| 2888638.664302826|\n",
            "|   11|2844694.1200847626|\n",
            "|    6|2831771.3922538757|\n",
            "|    4|2797452.9413547516|\n",
            "|    9|2763058.1299476624|\n",
            "|    2| 2647400.727924347|\n",
            "+-----+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####8. Category with Highest Sales:\n",
        "Which product category generated the highest total sales revenue?\n"
      ],
      "metadata": {
        "id": "YoiuB--48lZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_sales = sales_df.withColumn(\"Total_Price\", F.col(\"Price\") * F.col(\"Quantity\")) \\\n",
        "                         .groupBy(\"Category_Name\") \\\n",
        "                         .agg(F.sum(\"Total_Price\").alias(\"Total_Category_Sales\")) \\\n",
        "                         .orderBy(F.desc(\"Total_Category_Sales\")) \\\n",
        "                         .limit(1)\n",
        "\n",
        "category_sales.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9poXTNiN5PvY",
        "outputId": "cc691ec8-a373-4f86-9067-3fcc6decf1d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------------+\n",
            "|Category_Name|Total_Category_Sales|\n",
            "+-------------+--------------------+\n",
            "|      Fishing|   6929653.690338135|\n",
            "+-------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. State-wise Sales Comparison:\n",
        "Compare the total sales between two specific states (e.g., Texas vs. Ohio). Which state had higher sales?"
      ],
      "metadata": {
        "id": "TvGxhoAU9GD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "states_to_compare = [\"TX\", \"OH\"]  # Example states\n",
        "\n",
        "state_comparison = joined_df.filter(joined_df.State.isin(states_to_compare)) \\\n",
        "                            .withColumn(\"Total_Price\", F.col(\"Price\") * F.col(\"Quantity\")) \\\n",
        "                            .groupBy(\"State\") \\\n",
        "                            .agg(F.sum(\"Total_Price\").alias(\"Total_Sales\")) \\\n",
        "                            .orderBy(F.desc(\"Total_Sales\"))\n",
        "\n",
        "state_comparison.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvqbWrkO5PsD",
        "outputId": "1651c342-2b0e-4be2-84ad-017266a5c6fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------+\n",
            "|State|      Total_Sales|\n",
            "+-----+-----------------+\n",
            "|   TX|184629.3032875061|\n",
            "|   OH|82342.95152282715|\n",
            "+-----+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Detailed Customer Purchase Report:\n",
        "Generate a detailed report showing each customer along with their total purchases, the total number of transactions they have made, and the average transaction value.\n",
        "\n",
        "\n",
        "The issue might be here is that there is no `Name` column in `sales_df` but there is in `customers_df`. So we have just to join these two dataset to get the desired outcome of  \"Detailed Customer Purchase Report\" having both `Name` and `Customer_ID`.\n",
        "\n",
        "\n",
        "We can do it as flowing: ⚡\n"
      ],
      "metadata": {
        "id": "w2y53QVm9YEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify Join Condition ✅\n",
        "Ensure that both sales_df and customers_df have matching column names for the join key `Coustomer_ID`."
      ],
      "metadata": {
        "id": "XVnYagZ568gC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sales_df.columns)\n",
        "print(customers_df.columns)\n"
      ],
      "metadata": {
        "id": "IB62cDsi5Pad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "858a71c4-6af0-4eaa-d537-29c8dabd0244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sales_Txn_ID', 'Category_ID', 'Category_Name', 'Product_ID', 'Product_Name', 'Price', 'Quantity', 'Customer_ID', 'Transaction_Date', 'Month']\n",
            "['Customer_ID', 'Name', 'City', 'State', 'Zip_Code']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Specify The Join Type ✅\n",
        " Try a left join to retain all sales_df rows, even if no matching Name exists in customers_df."
      ],
      "metadata": {
        "id": "jGsmLHnG7YkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a left join to retain all rows from sales_df and match with customers_df\n",
        "joined_df = sales_df.join(customers_df, on=\"Customer_Id\", how=\"left\")\n"
      ],
      "metadata": {
        "id": "OUvMwBMg4xHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_report = joined_df.withColumn(\"Total_Price\", F.col(\"Price\") * F.col(\"Quantity\")) \\\n",
        "                           .groupBy(\"Customer_Id\", \"Name\") \\\n",
        "                           .agg(F.sum(\"Total_Price\").alias(\"Total_Purchases\"),\n",
        "                                F.count(\"Sales_Txn_Id\").alias(\"Total_Transactions\"),\n",
        "                                F.avg(\"Price\").alias(\"Avg_Transaction_Value\"))\n",
        "\n",
        "customer_report.show(5)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL3OeZqT4_ox",
        "outputId": "96c3dc60-58aa-4cb7-c8f3-861838a4fff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------+-----------------+------------------+---------------------+\n",
            "|Customer_Id|           Name|  Total_Purchases|Total_Transactions|Avg_Transaction_Value|\n",
            "+-----------+---------------+-----------------+------------------+---------------------+\n",
            "|       5534|Elizabeth Smith|3594.650074005127|                19|   154.98842440153422|\n",
            "|      11404|     Helen Cook|4017.410053253174|                21|   107.70333562578473|\n",
            "|       2669|   Dorothy Buck|5090.280059814453|                26|   115.44961797274075|\n",
            "|       5778|   Evelyn Smith|6587.310104370117|                28|   162.80821868351526|\n",
            "|       8530|  William Smith|379.8599967956543|                 3|    46.65000025431315|\n",
            "+-----------+---------------+-----------------+------------------+---------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h0k8Nk4J4_av"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}