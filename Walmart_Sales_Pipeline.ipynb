{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install PySpark and Java"
      ],
      "metadata": {
        "id": "nBydNUWyWEGc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkVyzjzNV6fi"
      },
      "outputs": [],
      "source": [
        "# Installing PySpark and Java\n",
        "# Installing  Java and the compatible version of PySpark\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download Spark (adjusting the version when needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "\n",
        "# Unzip the downloaded file\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buzyW7HFKH_m",
        "outputId": "50b9c29b-2904-4b54-d3d9-fd10aa8cbeac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the Environment Variables"
      ],
      "metadata": {
        "id": "f_qOuREgWY44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuring the environment for Java and Spark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n"
      ],
      "metadata": {
        "id": "LyiFbKrmV8CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark==3.1.2\n"
      ],
      "metadata": {
        "id": "rPP6CLUqZOt7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fbbe2f7-d150-4640-e435-a2c664edde0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.4/212.4 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "print(\"PySpark version:\", pyspark.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mr-hBIECX4Yu",
        "outputId": "e3ed4b47-8fdc-4afd-b65d-bec15a5b9af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PySpark version: 3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Spark Session"
      ],
      "metadata": {
        "id": "HweEofPUWz6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"WalmartSalesDataPipeline\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "Efdl_X9UV8Ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Data"
      ],
      "metadata": {
        "id": "ar_QGsixumbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data into DataFrames\n",
        "#customers_df = spark.read.csv(\"/content/customers.tsv\", header=True, inferSchema=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "rA3egHPCuZjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"AddHeadersToTSV\").getOrCreate()\n",
        "\n",
        "# Define the schema with the required column names and data types\n",
        "schema = StructType([\n",
        "    StructField(\"Customer ID\", StringType(), True),\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"City\", StringType(), True),\n",
        "    StructField(\"State\", StringType(), True),\n",
        "    StructField(\"Zip Code\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Load TSV data with the defined schema\n",
        "file_path = \"/content/customers.tsv\"\n",
        "customers_df = spark.read.option(\"delimiter\", \"\\t\").csv(file_path, schema=schema, header=False)\n",
        "\n",
        "# Show the result\n",
        "#df.show()\n"
      ],
      "metadata": {
        "id": "FuRBd3r7vWXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the result\n",
        "customers_df.show()"
      ],
      "metadata": {
        "id": "6qKyCaY6MbW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31170a1a-ce1c-42a8-f5ef-8040cedad16d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------+-------------+-----+--------+\n",
            "|Customer ID|            Name|         City|State|Zip Code|\n",
            "+-----------+----------------+-------------+-----+--------+\n",
            "|      11039|     Mary Torres|       Caguas|   PR|     725|\n",
            "|       5623|      Jose Haley|     Columbus|   OH|   43207|\n",
            "|       5829|      Mary Smith|      Houston|   TX|   77015|\n",
            "|       6336|  Richard Maddox|       Caguas|   PR|     725|\n",
            "|       1708|  Margaret Booth|    Arlington|   TX|   76010|\n",
            "|      10227|  Mary Henderson|       Caguas|   PR|     725|\n",
            "|        839|     Lisa Walker|       Caguas|   PR|     725|\n",
            "|       7604|   Jonathan Hill|      Phoenix|   AZ|   85040|\n",
            "|       6485|Carolyn Sheppard|Pompano Beach|   FL|   33063|\n",
            "|       4737|    Mary Mendoza|       Caguas|   PR|     725|\n",
            "|       5973|   Michael Smith|       Caguas|   PR|     725|\n",
            "|       9205|    James Holmes|     Hilliard|   OH|   43026|\n",
            "|        138|     Mary Dawson|       Caguas|   PR|     725|\n",
            "|        371|    Adam Marquez|  San Antonio|   TX|   78223|\n",
            "|       9285|    Gloria Smith|       Caguas|   PR|     725|\n",
            "|       1209|       Mary Webb|   San Marcos|   TX|   78666|\n",
            "|       3021|  Nancy Alvarado|     Flushing|   NY|   11354|\n",
            "|       3354|  Russell Flores|       Caguas|   PR|     725|\n",
            "|      11684|    Denise Smith|    Rego Park|   NY|   11374|\n",
            "|      11144|  Jose Dickerson|         Mesa|   AZ|   85201|\n",
            "+-----------+----------------+-------------+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sales_df = spark.read.csv(\"/content/salestxns.tsv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "Mle7Lf9TuUr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"AddHeadersToTSV\").getOrCreate()\n",
        "\n",
        "# Define the schema with the required column names and data types\n",
        "schema = StructType([\n",
        "    StructField(\"Sales Txn ID\", StringType(), True),\n",
        "    StructField(\"Category  ID\", StringType(), True),\n",
        "    StructField(\"Category Name\", StringType(), True),\n",
        "    StructField(\"Product  ID\", StringType(), True),\n",
        "    StructField(\"Product Name\", StringType(), True),\n",
        "    StructField(\"Price\", StringType(), True),\n",
        "    StructField(\"Quantity\", StringType(), True),\n",
        "    StructField(\"Customer ID\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Load TSV data with the defined schema\n",
        "file_path = \"/content/salestxns.tsv\"\n",
        "sales_df = spark.read.option(\"delimiter\", \"\\t\").csv(file_path, schema=schema, header=False)"
      ],
      "metadata": {
        "id": "RDTU-6FSyzq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.show()"
      ],
      "metadata": {
        "id": "W9OdfDbPMkcQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4629fa3-dcf0-4165-f945-adca28336197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------+--------------------+-----------+--------------------+------+--------+-----------+\n",
            "|Sales Txn ID|Category  ID|       Category Name|Product  ID|        Product Name| Price|Quantity|Customer ID|\n",
            "+------------+------------+--------------------+-----------+--------------------+------+--------+-----------+\n",
            "|           1|          43|    Camping & Hiking|        957|Diamondback Women...|299.98|       1|      11599|\n",
            "|           2|          48|        Water Sports|       1073|Pelican Sunstream...|199.99|       1|        256|\n",
            "|           3|          24|     Women's Apparel|        502|Nike Men's Dri-FI...|    50|       5|        256|\n",
            "|           4|          18|      Men's Footwear|        403|Nike Men's CJ Eli...|129.99|       1|        256|\n",
            "|           5|          40|         Accessories|        897|Team Golf New Eng...| 24.99|       2|       8827|\n",
            "|           6|          17|              Cleats|        365|Perfect Fitness P...| 59.99|       5|       8827|\n",
            "|           7|          24|     Women's Apparel|        502|Nike Men's Dri-FI...|    50|       3|       8827|\n",
            "|           8|          46|Indoor/Outdoor Games|       1014|O'Brien Men's Neo...| 49.98|       4|       8827|\n",
            "|           9|          43|    Camping & Hiking|        957|Diamondback Women...|299.98|       1|      11318|\n",
            "|          10|          17|              Cleats|        365|Perfect Fitness P...| 59.99|       5|      11318|\n",
            "|          11|          46|Indoor/Outdoor Games|       1014|O'Brien Men's Neo...| 49.98|       2|      11318|\n",
            "|          12|          43|    Camping & Hiking|        957|Diamondback Women...|299.98|       1|      11318|\n",
            "|          13|          18|      Men's Footwear|        403|Nike Men's CJ Eli...|129.99|       1|      11318|\n",
            "|          14|          48|        Water Sports|       1073|Pelican Sunstream...|199.99|       1|       4530|\n",
            "|          15|          43|    Camping & Hiking|        957|Diamondback Women...|299.98|       1|       4530|\n",
            "|          16|          41|            Trade-In|        926|Glove It Imperial...| 15.99|       5|       4530|\n",
            "|          17|          17|              Cleats|        365|Perfect Fitness P...| 59.99|       3|       2911|\n",
            "|          18|          17|              Cleats|        365|Perfect Fitness P...| 59.99|       5|       2911|\n",
            "|          19|          46|Indoor/Outdoor Games|       1014|O'Brien Men's Neo...| 49.98|       4|       2911|\n",
            "|          20|          24|     Women's Apparel|        502|Nike Men's Dri-FI...|    50|       1|       2911|\n",
            "+------------+------------+--------------------+-----------+--------------------+------+--------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing ✅\n",
        "\n",
        "#### Convert necessary columns to appropriate data types."
      ],
      "metadata": {
        "id": "Ay3v0rAi2OAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType, FloatType\n",
        "\n",
        "# Convert data types for necessary columns\n",
        "sales_df = sales_df.withColumn(\"Price\", sales_df[\"Price\"].cast(FloatType())) \\\n",
        "                   .withColumn(\"Quantity\", sales_df[\"Quantity\"].cast(IntegerType()))\n"
      ],
      "metadata": {
        "id": "rRSudtew0pyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SQL Queries"
      ],
      "metadata": {
        "id": "DBxHsO4W22Ft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Total Number of Customers:\n",
        "How many unique customers are there in the dataset?\n"
      ],
      "metadata": {
        "id": "N1lL4acD28zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_customers = customers_df.select(\"Customer ID\").distinct().count()\n",
        "print(\"Total number of unique customers:\", unique_customers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31MlHQGH0pdl",
        "outputId": "a6673c41-c3a7-421d-9391-53f5d7da3f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique customers: 1244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.Total Sales by State:\n",
        "What is the total sales amount for each state?"
      ],
      "metadata": {
        "id": "zgVsQ6JT3XP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Join sales and customers data\n",
        "joined_df = sales_df.join(customers_df, on=\"Customer ID\", how=\"outer\")\n",
        "\n",
        "# Calculate total sales by state\n",
        "total_sales_by_state = joined_df.withColumn(\"Total_Price\", F.col(\"Price\") * F.col(\"Quantity\")) \\\n",
        "                                .groupBy(\"State\") \\\n",
        "                                .agg(F.sum(\"Total_Price\").alias(\"Total_Sales\"))\n",
        "\n",
        "total_sales_by_state.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPhUILAy3WqJ",
        "outputId": "b4d50e3a-5380-4382-b785-3fd450520300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|State|         Total_Sales|\n",
            "+-----+--------------------+\n",
            "|   AZ|            48702.68|\n",
            "|   SC|             4144.68|\n",
            "|   LA|            24449.42|\n",
            "|   MN|  3549.6000000000004|\n",
            "|   NJ|  52303.090000000004|\n",
            "|   DC|             8798.76|\n",
            "|   OR|             9544.78|\n",
            "| null|3.0858028790000077E7|\n",
            "|   VA|  30488.970000000005|\n",
            "|   RI|   5424.410000000001|\n",
            "|   KY|              2749.7|\n",
            "|   MI|            83347.09|\n",
            "|   NV|   47103.60999999999|\n",
            "|   WI|             24561.3|\n",
            "|   ID|  10098.949999999997|\n",
            "|   CA|   503205.4899999998|\n",
            "|   CT|  19206.769999999997|\n",
            "|   NC|   45275.88999999999|\n",
            "|   MD|  51982.490000000005|\n",
            "|   DE|             1305.76|\n",
            "+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Top 10 Most Purchased Products:\n",
        "Which are the top 10 most purchased products based on the quantity sold?"
      ],
      "metadata": {
        "id": "9NMnCHEk5W-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_products = sales_df.groupBy(\"Product Name\") \\\n",
        "                       .agg(F.sum(\"Quantity\").alias(\"Total_Quantity\")) \\\n",
        "                       .orderBy(F.desc(\"Total_Quantity\")) \\\n",
        "                       .limit(10)\n",
        "\n",
        "top_products.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Eaxjppn5WfW",
        "outputId": "9db053d2-c200-4adf-9e17-3511e9758e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------+\n",
            "|        Product Name|Total_Quantity|\n",
            "+--------------------+--------------+\n",
            "|Perfect Fitness P...|       73698.0|\n",
            "|Nike Men's Dri-FI...|       62956.0|\n",
            "|O'Brien Men's Neo...|       57803.0|\n",
            "|Nike Men's Free 5...|       36680.0|\n",
            "|Under Armour Girl...|       31735.0|\n",
            "|Nike Men's CJ Eli...|       22246.0|\n",
            "|Field & Stream Sp...|       17325.0|\n",
            "|Pelican Sunstream...|       15500.0|\n",
            "|Diamondback Women...|       13729.0|\n",
            "|ENO Atlas Hammock...|         998.0|\n",
            "+--------------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Average Transaction Value:\n",
        "What is the average price of transactions across all sales?\n"
      ],
      "metadata": {
        "id": "Vph-eb6U56N6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_transaction_value = sales_df.withColumn(\"Total_Price\", F.col(\"Price\") * F.col(\"Quantity\")) \\\n",
        "                                .agg(F.avg(\"Total_Price\").alias(\"Avg_Transaction_Value\"))\n",
        "\n",
        "avg_transaction_value.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-8Hl08M5569",
        "outputId": "afdf6924-71f5-4963-c414-5cd330d62315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+\n",
            "|Avg_Transaction_Value|\n",
            "+---------------------+\n",
            "|   199.32066533882224|\n",
            "+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Top 5 Customers by Expenditure:\n",
        "Who are the top 5 customers by total amount spent?"
      ],
      "metadata": {
        "id": "jUAIkv8m6Nj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_customers = joined_df.withColumn(\"Total_Price\", F.col(\"Price\") * F.col(\"Quantity\")) \\\n",
        "                         .groupBy(\"Customer ID\", \"Name\") \\\n",
        "                         .agg(F.sum(\"Total_Price\").alias(\"Total_Expenditure\")) \\\n",
        "                         .orderBy(F.desc(\"Total_Expenditure\")) \\\n",
        "                         .limit(5)\n",
        "\n",
        "top_customers.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te10n59P6HSo",
        "outputId": "d9788a7c-b957-472e-b670-a1f34fd64ec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+------------------+\n",
            "|Customer ID|          Name| Total_Expenditure|\n",
            "+-----------+--------------+------------------+\n",
            "|        791|          null|10524.169999999993|\n",
            "|       9371|Mary Patterson| 9299.029999999997|\n",
            "|       8766|          null| 9296.139999999998|\n",
            "|       1657|          null| 9223.709999999994|\n",
            "|       2641|          null| 9130.919999999995|\n",
            "+-----------+--------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Product Purchases by a Specific Customer:\n",
        "List all products purchased by a specific customer (e.g., customer with ID 256)"
      ],
      "metadata": {
        "id": "pt8KIeLQ6jPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_id = 256  # Example Customer ID\n",
        "\n",
        "customer_purchases = sales_df.filter(sales_df[\"Customer ID\"] == customer_id) \\\n",
        "                             .select(\"Product Name\", \"Quantity\", (F.col(\"Price\") * F.col(\"Quantity\")).alias(\"Total_Spent\"))\n",
        "\n",
        "customer_purchases.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaciZUe26G8M",
        "outputId": "05f62338-0f2b-4e90-b819-2fff81c457c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------+------------------+\n",
            "|        Product Name|Quantity|       Total_Spent|\n",
            "+--------------------+--------+------------------+\n",
            "|Pelican Sunstream...|       1|            199.99|\n",
            "|Nike Men's Dri-FI...|       5|             250.0|\n",
            "|Nike Men's CJ Eli...|       1|            129.99|\n",
            "|Team Golf St. Lou...|       5|124.94999999999999|\n",
            "|TYR Boys' Team Di...|       5|199.95000000000002|\n",
            "|Field & Stream Sp...|       1|            399.98|\n",
            "|Field & Stream Sp...|       1|            399.98|\n",
            "|Nike Men's Dri-FI...|       5|             250.0|\n",
            "|Nike Men's CJ Eli...|       1|            129.99|\n",
            "|Nike Men's CJ Eli...|       1|            129.99|\n",
            "|Perfect Fitness P...|       5|            299.95|\n",
            "|O'Brien Men's Neo...|       5|249.89999999999998|\n",
            "|Nike Men's CJ Eli...|       1|            129.99|\n",
            "|O'Brien Men's Neo...|       4|            199.92|\n",
            "|Under Armour Wome...|       1|             54.97|\n",
            "|Nike Women's Temp...|       4|             120.0|\n",
            "|Nike Men's Dri-FI...|       1|              50.0|\n",
            "|Nike Men's Dri-FI...|       2|             100.0|\n",
            "|Nike Men's Dri-FI...|       2|             100.0|\n",
            "|Diamondback Women...|       1|            299.98|\n",
            "+--------------------+--------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Monthly Sales Trends:\n",
        "Assuming there is a date field, analyze the sales trends over the months. Which month had the highest sales?\n"
      ],
      "metadata": {
        "id": "50vRjkum7ifV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Since There is no 'Date' and 'Month' data column available in `Sales_df` dataset, so we can not extract Monthly Sales Data from it .\n",
        "\n",
        "- Since the document doesn’t indicate the presence of a date field in `sales_df`, we need to add a date column manually .\n",
        "\n",
        "- We will generate random dates within a specified range for each transaction, we can add a `Transaction_Date` column with dates spread randomly over a given range.\n",
        "\n",
        "\n",
        "So What the things we need to able to get the desired output...? ☝\n",
        "\n",
        "\n",
        "We can implement three things here:\n",
        "\n",
        "\n",
        "1) **Define the Date Range** Choose a start and end date.\n",
        "\n",
        "2)**Generate Random Dates** within that range for each row.\n",
        "\n",
        "3)**Extract the Month from the random** dates for monthly analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "3QbNM3vIBupu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import DateType\n",
        "from datetime import date, timedelta\n",
        "\n",
        "# Define the start and end dates for the random date range\n",
        "start_date = date(2023, 1, 1)\n",
        "end_date = date(2023, 12, 31)\n",
        "\n",
        "# Function to generate a random date within the specified range\n",
        "def random_date(start, end):\n",
        "    delta = end - start\n",
        "    random_days = random.randint(0, delta.days)\n",
        "    return start + timedelta(days=random_days)\n",
        "\n",
        "# Register the function as a UDF\n",
        "random_date_udf = F.udf(lambda: random_date(start_date, end_date), DateType())   # UDF = User Defined Function\n",
        "\n",
        "# Add a Transaction_Date column with random dates for each transaction\n",
        "sales_df = sales_df.withColumn(\"Transaction_Date\", random_date_udf())\n",
        "\n",
        "# Extract the month from the new Transaction_Date column\n",
        "sales_df = sales_df.withColumn(\"Month\", F.month(\"Transaction_Date\"))\n",
        "\n",
        "# Calculate monthly sales\n",
        "monthly_sales = sales_df.withColumn(\"Total_Price\", F.col(\"Price\") * F.col(\"Quantity\")) \\\n",
        "                        .groupBy(\"Month\") \\\n",
        "                        .agg(F.sum(\"Total_Price\").alias(\"Monthly_Sales\")) \\\n",
        "                        .orderBy(F.desc(\"Monthly_Sales\"))\n",
        "\n",
        "# Show the result\n",
        "monthly_sales.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-P2eVQErBjSL",
        "outputId": "13eba6fa-9a6d-43dd-d453-d958edfba37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------------+\n",
            "|Month|     Monthly_Sales|\n",
            "+-----+------------------+\n",
            "|    7| 2940069.659999812|\n",
            "|    8|  2926173.94999981|\n",
            "|    1| 2924858.619999815|\n",
            "|   10|2910291.5399998133|\n",
            "|    5|2897295.5099998154|\n",
            "|    3| 2875720.519999815|\n",
            "|   12|2860295.8299998157|\n",
            "|   11| 2848909.149999821|\n",
            "|    9|2847548.4699998177|\n",
            "|    4|  2831549.59999982|\n",
            "|    6|  2826857.76999982|\n",
            "|    2|2633049.3099998385|\n",
            "+-----+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####8. Category with Highest Sales:\n",
        "Which product category generated the highest total sales revenue?\n"
      ],
      "metadata": {
        "id": "YoiuB--48lZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_sales = sales_df.withColumn(\"Total_Price\", F.col(\"Price\") * F.col(\"Quantity\")) \\\n",
        "                         .groupBy(\"Category Name\") \\\n",
        "                         .agg(F.sum(\"Total_Price\").alias(\"Total_Category_Sales\")) \\\n",
        "                         .orderBy(F.desc(\"Total_Category_Sales\")) \\\n",
        "                         .limit(1)\n",
        "\n",
        "category_sales.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9poXTNiN5PvY",
        "outputId": "f44ddb76-554b-487d-c6aa-b5e864bec417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------------+\n",
            "|Category Name|Total_Category_Sales|\n",
            "+-------------+--------------------+\n",
            "|      Fishing|   6929653.500000114|\n",
            "+-------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. State-wise Sales Comparison:\n",
        "Compare the total sales between two specific states (e.g., Texas vs. Ohio). Which state had higher sales?"
      ],
      "metadata": {
        "id": "TvGxhoAU9GD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "states_to_compare = [\"TX\", \"OH\"]  # Example states\n",
        "\n",
        "state_comparison = joined_df.filter(joined_df.State.isin(states_to_compare)) \\\n",
        "                            .withColumn(\"Total_Price\", F.col(\"Price\") * F.col(\"Quantity\")) \\\n",
        "                            .groupBy(\"State\") \\\n",
        "                            .agg(F.sum(\"Total_Price\").alias(\"Total_Sales\")) \\\n",
        "                            .orderBy(F.desc(\"Total_Sales\"))\n",
        "\n",
        "state_comparison.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvqbWrkO5PsD",
        "outputId": "0a705f51-c69c-468d-fc4c-b482b941e70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------------+\n",
            "|State|       Total_Sales|\n",
            "+-----+------------------+\n",
            "|   TX|184629.30000000045|\n",
            "|   OH| 82342.95000000014|\n",
            "+-----+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Detailed Customer Purchase Report:\n",
        "Generate a detailed report showing each customer along with their total purchases, the total number of transactions they have made, and the average transaction value.\n"
      ],
      "metadata": {
        "id": "w2y53QVm9YEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_report = joined_df.withColumn(\"Total_Price\", F.col(\"Price\") * F.col(\"Quantity\")) \\\n",
        "                           .groupBy(\"Customer ID\", \"Name\") \\\n",
        "                           .agg(F.sum(\"Total_Price\").alias(\"Total_Purchases\"),\n",
        "                                F.count(\"Sales Txn ID\").alias(\"Total_Transactions\"),\n",
        "                                F.avg(\"Price\").alias(\"Avg_Transaction_Value\"))\n",
        "\n",
        "customer_report.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNqYbXkH5Pdx",
        "outputId": "b6304ca0-8748-4eea-e8f4-ca4675b08947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----+------------------+------------------+---------------------+\n",
            "|Customer ID|Name|   Total_Purchases|Total_Transactions|Avg_Transaction_Value|\n",
            "+-----------+----+------------------+------------------+---------------------+\n",
            "|      10096|null|           2211.76|                11|   158.53272727272727|\n",
            "|      10351|null| 8339.259999999998|                28|    212.4871428571428|\n",
            "|      10436|null|2484.6299999999997|                16|   112.80125000000001|\n",
            "|       1090|null|1387.8100000000002|                 7|   141.70285714285714|\n",
            "|      11078|null| 919.9000000000001|                 4|              84.9925|\n",
            "|      11332|null|3295.6399999999994|                22|   108.71818181818183|\n",
            "|      11563|null|2619.7100000000005|                16|   118.11562500000001|\n",
            "|       1159|null|3567.6500000000005|                19|   139.25052631578947|\n",
            "|      11722|null|           1529.81|                 9|    71.65777777777778|\n",
            "|      12394|null|3882.5199999999995|                20|             142.5375|\n",
            "|       1436|null|4419.5599999999995|                17|   189.39823529411765|\n",
            "|       1512|null| 5614.419999999997|                30|   146.48799999999994|\n",
            "|       1572|null|2779.7400000000002|                11|   179.98636363636365|\n",
            "|       2088|null|           3109.66|                14|               143.56|\n",
            "|       2136|null|2307.7300000000005|                11|    153.4427272727273|\n",
            "|       2162|null|1549.8100000000002|                 5|              139.988|\n",
            "|       2294|null|            1849.7|                11|   117.08000000000001|\n",
            "|       2904|null|2979.6099999999997|                13|   125.37307692307694|\n",
            "|        296|null|            4559.4|                23|   133.03130434782605|\n",
            "|       3210|null|            899.95|                 4|               159.99|\n",
            "+-----------+----+------------------+------------------+---------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IB62cDsi5Pad"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}